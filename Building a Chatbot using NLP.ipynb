{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human beings communicate among themselves using language. Language is made of alphabets and grammar. If we've to communicate in the same way with a computer, we've to make him understand these alphabets and grammar. If we were expecting an answer from computer on - \"What's the weather like outside?\". We've to first teach him the meaning of the words used in this sentence and also the context in which these words are used. Also, once computer understands the question, it should be able to generate an appropriate response. So, the process of understaing the language and generating it is known as Natural Language Processing.\n",
    "\n",
    "This is mainly used for processing unstructured data. Unstructured data is the data that is generated from text messages, tweets, blogs, images. Structured data is the one that is organized and neatly fit into the defined schema. For example, data in Relational Databases. Around 80% of the data is unstructured and we need a means to process this data to make inference out of it. NLP deals with this unstructured data (Mostly text)\n",
    "\n",
    "NLP can be of two types - Natural Language Understanding (NLU) and Natural Language Generation (NLG). We're building Chatbot using Natural Language Toolkit. The basic format to process data is \n",
    "1. Tokenization\n",
    "2. Stemming and Lemmatization\n",
    "\n",
    "Tokenization means breaking text sentences into sentences and words. \n",
    "Stemming means to convert words into some base word. This base word may not be a proper word. For example - Stemming of words give, given, giving results in giv and giv is not a word in English dictionary.\n",
    "Lemmatization is the same process as that of Stemming. However, Lemmatization results in proper word of the dictionary. \n",
    "\n",
    "We've to run our text through these processes to weigh the words used in the sentence. We're going to build a chatbot using NLTK and SciKit packages. Let's first import the basic packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk is Natural Language Toolkit\n",
    "numpy is used for faster execution of array operations\n",
    "We'll be using random to generate a random choice and string is imported because we'll be using some of its basics methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbots_file = open('chatbot.txt', 'r', errors='ignore')\n",
    "content = chatbots_file.read()\n",
    "content = content.lower()\n",
    "\n",
    "# Comment this after first time download\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sentence_tokens = nltk.sent_tokenize(content)\n",
    "word_tokens = nltk.word_tokenize(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, We're using chatbots data from wikipedia and is directly dumped into chatbot.text file. We're reading that file using the read method and then storing its entire content in lower case in the content variable. NLTK comes bundled with a lot of models (For example, it has models containing data on movie reviews and ratings). We're using its punkt and wordnet models. punkt is a pre-trained tokenizer for English. Wordnet is a lexical database for the English language created by Princeton. It can be used to find the meanings of words, synonyms, antonynms.\n",
    "\n",
    "sent_tokenize method converts our content into sentences and word_tokenize method converts it into words. This may sound simple if we assume that the sentence tokenizer splits the text based on the occurrence of a period. But that is not correct. It is smart enough to identify the period in \"Mr. John is a wise man. He wakes up early in the morning.\" It does't consider the dot after Mr as another sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a WordNetLemmatizer for finding the lemma of words used in our content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_tokens (tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above sentence is used for removing the unnecessary noise created by the punctuation marks. We're putting None if any punctuation marks are present in the sentence. Let's see what we get when we print string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_normalize (text):\n",
    "    return lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `greeting` method below returns a random response from the `GREETING_RESPONSES` list if the input is anyone of the `GREETING_INPUTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting (sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using scikit-learn library for generating the TF-IDF values. TF-IDF is Term Frequency-Inverse Document Frequency value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the `response` method below as of now. We'll get into its details in some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response (user_response):\n",
    "    robo_response = ''\n",
    "    sentence_tokens.append(user_response)\n",
    "    TfidVec = TfidfVectorizer(tokenizer = lem_normalize, stop_words = 'english')\n",
    "    tfidf = TfidVec.fit_transform(sentence_tokens)\n",
    "    values = cosine_similarity(tfidf[-1], tfidf)\n",
    "    print('values in cosine_smililarity', values)\n",
    "    idx = values.argsort()[0][-2]\n",
    "    print('argsort', values.argsort())\n",
    "    flat = values.flatten()\n",
    "    flat.sort()\n",
    "    print('after flattening', flat)\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if (req_tfidf == 0):\n",
    "        robo_response = robo_response + 'I\\'m sorry! I don\\'t understand you.'\n",
    "    else:\n",
    "        robo_response = robo_response + sentence_tokens[idx]\n",
    "        \n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to implement a mechanism to take input from the user, process that input and send back the relevant response. `input()` renders a input field and whatever the user types in that field in stored in the variable `user_response`. We convert the user's input in lower case to match the content that we have (Remember, we've converted our entire chatbots content in lower case). We've to end the loop of conversation between bot and user when the user types `bye`. Check the if condition that evalutes if the input is either `thanks` or `thank you`. It then terminates the loop by saying `You are welcome..`. If the input is any of our `GREETING_INPUTS`, we send back any random value from the `GREETING_RESPONSES` list. This logic is implemented in the `greeting` method above. Now, lets say user has asked a question to the bot. We call the `response` method from above and render its output as the answer to the user's questions.\n",
    "\n",
    "Now, its time to understand what our `response` method actually does.\n",
    "\n",
    "We append the questions of the user (stored in `user_response` variable) to `sentence_tokens` (`sentence_tokens` contains sentence tokens from our chatbots content). As stated above, TF-IDF is Term Frequency Inverse Document Frequency. Before getting into its details, we've to first understand how the bot would respond to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = True\n",
    "gprint(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "\n",
    "while (flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    \n",
    "    if (user_response != 'bye'):\n",
    "        if (user_response == 'thanks' or user_response == 'thank you'):\n",
    "            flag = False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if (greeting(user_response) != None):\n",
    "                print(\"ROBO: \" + greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \")\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who was noted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
