{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human beings communicate among themselves using language. Language is made of alphabets and grammar. If we've to communicate in the same way with a computer, we've to make him understand these alphabets and grammar. If we were expecting an answer from computer on - \"What's the weather like outside?\". We've to first teach him the meaning of the words used in this sentence and also the context in which these words are used. Also, once computer understands the question, it should be able to generate an appropriate response. So, the process of understaing the language and generating it is known as Natural Language Processing.\n",
    "\n",
    "This is mainly used for processing unstructured data. Unstructured data is the data that is generated from text messages, tweets, blogs, images. Structured data is the one that is organized and neatly fit into the defined schema. For example, data in Relational Databases. Around 80% of the data is unstructured and we need a means to process this data to make inference out of it. NLP deals with this unstructured data (Mostly text)\n",
    "\n",
    "NLP can be of two types - Natural Language Understanding (NLU) and Natural Language Generation (NLG). We're building Chatbot using Natural Language Toolkit. The basic format to process data is \n",
    "1. Tokenization\n",
    "2. Stemming and Lemmatization\n",
    "\n",
    "Tokenization means breaking text sentences into sentences and words. \n",
    "Stemming means to convert words into some base word. This base word may not be a proper word. For example - Stemming of words give, given, giving results in giv and giv is not a word in English dictionary.\n",
    "Lemmatization is the same process as that of Stemming. However, Lemmatization results in proper word of the dictionary. \n",
    "\n",
    "We've to run our text through these processes to weigh the words used in the sentence. We're going to build a chatbot using NLTK and SciKit packages. Let's first import the basic packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk is Natural Language Toolkit\n",
    "numpy is used for faster execution of array operations\n",
    "We'll be using random to generate a random choice and string is imported because we'll be using some of its basics methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gaurav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gaurav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "chatbots_file = open('chatbot.txt', 'r', errors='ignore')\n",
    "content = chatbots_file.read()\n",
    "content = content.lower()\n",
    "\n",
    "# UnComment this after first time download\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sentence_tokens = nltk.sent_tokenize(content)\n",
    "word_tokens = nltk.word_tokenize(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, We're using chatbots data from wikipedia and is directly dumped into chatbot.text file. We're reading that file using the read method and then storing its entire content in lower case in the content variable. NLTK comes bundled with a lot of models (For example, it has models containing data on movie reviews and ratings). We're using its punkt and wordnet models. punkt is a pre-trained tokenizer for English. Wordnet is a lexical database for the English language created by Princeton. It can be used to find the meanings of words, synonyms, antonynms.\n",
    "\n",
    "sent_tokenize method converts our content into sentences and word_tokenize method converts it into words. This may sound simple if we assume that the sentence tokenizer splits the text based on the occurrence of a period. But that is not correct. It is smart enough to identify the period in \"Mr. John is a wise man. He wakes up early in the morning.\" It does't consider the dot after Mr as another sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a WordNetLemmatizer for finding the lemma of words used in our content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_tokens (tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above sentence is used for removing the unnecessary noise created by the punctuation marks. We're putting None if any punctuation marks are present in the sentence. Let's see what we get when we print string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_normalize (text):\n",
    "    return lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `greeting` method below returns a random response from the `GREETING_RESPONSES` list if the input is anyone of the `GREETING_INPUTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting (sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using scikit-learn library for generating the TF-IDF values. TF-IDF is Term Frequency-Inverse Document Frequency value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the `response` method below as of now. We'll get into its details in some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response (user_response):\n",
    "    robo_response = ''\n",
    "    sentence_tokens.append(user_response)\n",
    "    TfidVec = TfidfVectorizer(tokenizer = lem_normalize, stop_words = 'english')\n",
    "    tfidf = TfidVec.fit_transform(sentence_tokens)\n",
    "    \n",
    "    values = cosine_similarity(tfidf[-1], tfidf)\n",
    "    \n",
    "    idx = values.argsort()[0][-2]\n",
    "    flat = values.flatten()\n",
    "    flat.sort()\n",
    "    \n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if (req_tfidf == 0):\n",
    "        robo_response = robo_response + 'I\\'m sorry! I don\\'t understand you.'\n",
    "    else:\n",
    "        robo_response = robo_response + sentence_tokens[idx]\n",
    "        \n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to implement a mechanism to take input from the user, process that input and send back the relevant response. `input()` renders a input field and whatever the user types in that field in stored in the variable `user_response`. We convert the user's input in lower case to match the content that we have (Remember, we've converted our entire chatbots content in lower case). We've to end the loop of conversation between bot and user when the user types `bye`. Check the if condition that evalutes if the input is either `thanks` or `thank you`. It then terminates the loop by saying `You are welcome..`. If the input is any of our `GREETING_INPUTS`, we send back any random value from the `GREETING_RESPONSES` list. This logic is implemented in the `greeting` method above. Now, lets say user has asked a question to the bot. We call the `response` method from above and render its output as the answer to the user's questions.\n",
    "\n",
    "Now, its time to understand what our `response` method actually does.\n",
    "\n",
    "We append the questions of the user (stored in `user_response` variable) to `sentence_tokens` (`sentence_tokens` contains sentence tokens from our chatbots content). As stated above, TF-IDF is Term Frequency Inverse Document Frequency. Before getting into its details, we've to first understand how the bot would respond to the user's query.\n",
    "\n",
    "The single source of truth for Bot is the `chatbot.txt` file. That's all the bot knows. It'll search that document for answering our questions. When we ask - `What is a turing machine?`. It'll find all the relevant sentences that mention turing machine and will return one of them. In this process, it has to make sure that it puts weights on words appropriately. Like for this question, if it starts searching for `what` and `is`, it wouldn't make sense. It should be searching for `turing machine`. Now, that's the task that we've to handle - to weigh down the words. \n",
    "\n",
    "If we were to do this by simple logic of calculating the frequency of words (This is called as Bag of Words), that wouldn't be appt. To weigh our words appropriately, we're using TF-IDF.\n",
    "\n",
    "TF (Term Frequency) = Number of time a term appears in a document / Total number of terms in the document\n",
    "IDF (Inverse Document Frequency) = log ( Total number of documents / Number of documents with that term in it)\n",
    "\n",
    "TF-IDF = TF*IDF\n",
    "\n",
    "We're using `TfidfVectorizer` to convert our document to a matrix of TF-IDF features. \n",
    "\n",
    "`TfidVec = TfidfVectorizer(tokenizer = lem_normalize, stop_words = 'english')` By supplying the `tokenizer` as `lem_normalize`, we're overriding its default way of lemmatizing tokens. As mentioned above, there are some words in English like `what` `is` that are of low importance and should be ignored while generating a response. `stop_words` contains an extensive list of such words in English. These words will be assigned a lowest value of 0 in the resulting TF-IDF matrix. Now that we initialized our `TfidVec`, we'll be providing it our document. \n",
    "`tfidf = TfidVec.fit_transform(sentence_tokens)` returns a tf-idf matrix. \n",
    "\n",
    "Next, `values = cosine_similarity(tfidf[-1], tfidf)`. `cosine_similarity` takes in two TF-IDF vectors and computes similarity between these two. The two arguments provided to `cosine_similarity` here are `tfidf[-1]` - The last element in our tfidf matrix (Remember we've appended the user's question to our sentence_tokens, that's the first argument) and `tfidf` is the entire matrix. `cosine_similarity` measures the cosine of the angle between these two vectors. It takes into consideration the orientation of the vectors and not their magnitude. An orintation of 0 degrees means the vectors are parallel and this would result in cosine of 0 deg as 1, which suggests that the vectors are similar. Cosine of positive space would result in values from 0 to 1. So, two vectors that are 90deg apart would result in cosine of 0 and that suggests there is nothing common in these two vectors. \n",
    "\n",
    "`argsort` returns an array with the indices of elements in sorted form. So, if we've an array like [2, 1, 4] argsort of this array would be [1, 0, 2] The first element 1 is the index of element 1 in our original array and 0 is the index of 2 in the original array. `flatten` converts it into 1-D array and then we're sorting it based on the values of TF-IDF. We consider the second last value in this array as the most appropriate response. The last value will be the input that user has entered. Second last value will have the max TF-IDF value. If that value is 0, we straight away print `I'm sorry! I don't understand you.` And if it has some definite value, we print the text on that index. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "hi\n",
      "ROBO: hi\n",
      "what is turing machine\n",
      "ROBO: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background\n",
      "in 1950, alan turing's famous article \"computing machinery and intelligence\" was published, which proposed what is now called the turing test as a criterion of intelligence.\n",
      "bye\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "\n",
    "while (flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    \n",
    "    if (user_response != 'bye'):\n",
    "        if (user_response == 'thanks' or user_response == 'thank you'):\n",
    "            flag = False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if (greeting(user_response) != None):\n",
    "                print(\"ROBO: \" + greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \")\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
